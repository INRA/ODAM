{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ODAM: Deployment and User's Guide \u00b6 ODAM (Open Data for Access and Mining) is an Experiment Data Table Management System (EDTMS) ODAM (Open Data for Access and Mining) is an Experimental Data Table Management System (EDTMS) that implements a simple and effective way to make research data widely accessible and fully available for reuse, and this with minimal effort on the part of the data provider, and allows any scientist or data researcher to be able to explore the dataset and then extract some or all of the data according to their needs. For full documentation read the ODAM - Deployment and User's Guide","title":"Home"},{"location":"#odam-deployment-and-users-guide","text":"ODAM (Open Data for Access and Mining) is an Experiment Data Table Management System (EDTMS) ODAM (Open Data for Access and Mining) is an Experimental Data Table Management System (EDTMS) that implements a simple and effective way to make research data widely accessible and fully available for reuse, and this with minimal effort on the part of the data provider, and allows any scientist or data researcher to be able to explore the dataset and then extract some or all of the data according to their needs. For full documentation read the ODAM - Deployment and User's Guide","title":"ODAM: Deployment and User's Guide"},{"location":"about/","text":"ODAM: Deployment and User's Guide \u00b6 (C) Daniel Jacob - INRAE UMR 1332 BFP, PMB - 2020 - Version 1.1 Set of tools and protocols implemented in this work Description Type Link Data Preparation Protocol for ODAM Compliance Documentation doi:10.17504/protocols.io.betcjeiw API Documentation based on Swagger Web API Tool INRA-PMB/ODAM/1.0.1-oas3/ R ODAM package and How to use it Package https://cran.r-project.org/package=Rodam R ODAM package Vignette Package Rodam/vignettes/Rodam.html Virtual Machine embedding the ODAM software on Oracle VM VirtualBox along with its installation guide Virtual Machine + Documentation https://doi.org/10.15454/C9LAEF Docker containers on DockerHub for installing on a Linux machine Container https://hub.docker.com/r/odam/getdata/ https://hub.docker.com/r/odam/dataexplorer/ A very lightweight local web server for Windows to deploy the ODAM API Web API Tool http://pmb-bordeaux.fr/odam/ODAMwebserver/ Examples of Jupyter notebooks (R & Python) based on the ODAM Web API Notebook https://github.com/djacob65/binder_odam https://doi.org/10.24433/CO.8981049.v1 https://doi.org/10.24433/CO.0011270.v1 Modeling the growth of tomato fruits based on enzyme activity profiles (Example of data analysis interfaced by ODAM) Notebook https://hal.inrae.fr/hal-02611223 ODAM Source code on GitHub Source Code https://github.com/inrae/ODAM JSON Schema for ODAM data package Source Code https://github.com/djacob65/odam-datapackage/","title":"About"},{"location":"about/#odam-deployment-and-users-guide","text":"(C) Daniel Jacob - INRAE UMR 1332 BFP, PMB - 2020 - Version 1.1 Set of tools and protocols implemented in this work Description Type Link Data Preparation Protocol for ODAM Compliance Documentation doi:10.17504/protocols.io.betcjeiw API Documentation based on Swagger Web API Tool INRA-PMB/ODAM/1.0.1-oas3/ R ODAM package and How to use it Package https://cran.r-project.org/package=Rodam R ODAM package Vignette Package Rodam/vignettes/Rodam.html Virtual Machine embedding the ODAM software on Oracle VM VirtualBox along with its installation guide Virtual Machine + Documentation https://doi.org/10.15454/C9LAEF Docker containers on DockerHub for installing on a Linux machine Container https://hub.docker.com/r/odam/getdata/ https://hub.docker.com/r/odam/dataexplorer/ A very lightweight local web server for Windows to deploy the ODAM API Web API Tool http://pmb-bordeaux.fr/odam/ODAMwebserver/ Examples of Jupyter notebooks (R & Python) based on the ODAM Web API Notebook https://github.com/djacob65/binder_odam https://doi.org/10.24433/CO.8981049.v1 https://doi.org/10.24433/CO.0011270.v1 Modeling the growth of tomato fruits based on enzyme activity profiles (Example of data analysis interfaced by ODAM) Notebook https://hal.inrae.fr/hal-02611223 ODAM Source code on GitHub Source Code https://github.com/inrae/ODAM JSON Schema for ODAM data package Source Code https://github.com/djacob65/odam-datapackage/","title":"ODAM: Deployment and User's Guide"},{"location":"api/","text":"ODAM: Deployment and User's Guide \u00b6 Web API \u00b6 Based on REST services using a Resource Naming convention: an understandable resource naming leading to an easily leveraged Web service API (Identification/querying of resources) and easy to implement within R. Output formats : TSV, JSON and XML. Even if Web API outputs are not dedicated to human readers (script languages as R are the typical clients), XML outputs can be human readable in a web browser, made possible by using a XSL transformation mechanism which converts the XML outputs to HTML format. Using the two metadata files, it is possible to build a tree structure from which the data files can be queried to extract a subset. The tree structure is built on the Entity-Attribute-Value scheme. Field Description Examples <data format> Format of the retrieved data; possible values are: 'xml', \u2018json\u2019 or \u2018tsv' tsv <dataset name> Short name (tag) of your dataset frim1 <subset> Short name of a data subset samples <entry> Name of an attribute entry (defined by the user in the a_attribute file (column \u2018entry\u2019) sampleid <category> Name of the attribute category; (assigned by the user in the a_attribute file (column \u2018category\u2019). Possible values are: \u2018identifier\u2019, \u2018factor\u2019, \u2018qualitative\u2019, \u2018quantitative\u2019 quantitative (<subset>) Set of data subsets by merging all the subsets with lower rank than the specified subset and following the pathway defined by the \"is_part_of\" links. (samples) <=> plants + samples <value> Exact value of the desired entry or category 1 (for subset) or Factor (for category) API Documentation on SwaggerHub : INRA-PMB/ODAM/1.0.1-oas3","title":"Web API"},{"location":"api/#odam-deployment-and-users-guide","text":"","title":"ODAM: Deployment and User's Guide"},{"location":"api/#web-api","text":"Based on REST services using a Resource Naming convention: an understandable resource naming leading to an easily leveraged Web service API (Identification/querying of resources) and easy to implement within R. Output formats : TSV, JSON and XML. Even if Web API outputs are not dedicated to human readers (script languages as R are the typical clients), XML outputs can be human readable in a web browser, made possible by using a XSL transformation mechanism which converts the XML outputs to HTML format. Using the two metadata files, it is possible to build a tree structure from which the data files can be queried to extract a subset. The tree structure is built on the Entity-Attribute-Value scheme. Field Description Examples <data format> Format of the retrieved data; possible values are: 'xml', \u2018json\u2019 or \u2018tsv' tsv <dataset name> Short name (tag) of your dataset frim1 <subset> Short name of a data subset samples <entry> Name of an attribute entry (defined by the user in the a_attribute file (column \u2018entry\u2019) sampleid <category> Name of the attribute category; (assigned by the user in the a_attribute file (column \u2018category\u2019). Possible values are: \u2018identifier\u2019, \u2018factor\u2019, \u2018qualitative\u2019, \u2018quantitative\u2019 quantitative (<subset>) Set of data subsets by merging all the subsets with lower rank than the specified subset and following the pathway defined by the \"is_part_of\" links. (samples) <=> plants + samples <value> Exact value of the desired entry or category 1 (for subset) or Factor (for category) API Documentation on SwaggerHub : INRA-PMB/ODAM/1.0.1-oas3","title":"Web API"},{"location":"data_explorer/","text":"ODAM: Deployment and User's Guide \u00b6 Data Explorer \u00b6 The Data Explorer makes data easy to explore, visualize, and subsequently to better understand the data as a whole. Explore your data in several ways according to your concerns by interacting with the graphs. For instance, univariate, bivariate and multivariate approaches have been implemented so that they are very easy to be interactively used. This is very useful in order to have a first glimpse of the data that can show trends and this allows the data to be well characterized, which is necessary to then choose how to analyze it later on. The Data Explorer Example of graphical output produced by the Data Explorer For each analysis, several possibilities can be explored by selecting many parameters and interacting with the graphs. Graphics can be easily exported as PNG files. An interesting possibility is that you can make a selection of a subset of data and then export it directly into your spreadsheet. Export a previously selected data subset.","title":"Data Explorer"},{"location":"data_explorer/#odam-deployment-and-users-guide","text":"","title":"ODAM: Deployment and User's Guide"},{"location":"data_explorer/#data-explorer","text":"The Data Explorer makes data easy to explore, visualize, and subsequently to better understand the data as a whole. Explore your data in several ways according to your concerns by interacting with the graphs. For instance, univariate, bivariate and multivariate approaches have been implemented so that they are very easy to be interactively used. This is very useful in order to have a first glimpse of the data that can show trends and this allows the data to be well characterized, which is necessary to then choose how to analyze it later on. The Data Explorer Example of graphical output produced by the Data Explorer For each analysis, several possibilities can be explored by selecting many parameters and interacting with the graphs. Graphics can be easily exported as PNG files. An interesting possibility is that you can make a selection of a subset of data and then export it directly into your spreadsheet. Export a previously selected data subset.","title":"Data Explorer"},{"location":"data_type/","text":"ODAM: Deployment and User's Guide \u00b6 Data collection and preparation \u00b6 Data Type \u00b6 Whatever the kind of experiment, this assumes a design of experiment ( DoE ) involving individuals, samples or whatever things, as the main objects of study and producing several experimental data tables . This also assumes the observation of dependent variables resulting from effects of some controlled independent variables ( factors 1 ). Moreover, the objects of study usually have an identifier 2 for each of them, and the variables can be quantitative 3 or qualitative 4 . Example of an experiment data table viewed according to the repartition by category as introduced in the text Data management : promote good practices \u00b6 First and foremost, it is important to have well-organized data. The files generated during data collection have to be organized according to the entity-attribute relational model. Indeed, each entity corresponds to a type of collected data (samples, compounds, ...) for which is associated a set of attributes, i.e. observed or measured variables. Well organized data means that each variable forms a column, each observation forms a line, and each type of \"unit observational\" forms a table, i.e a file. Then, a link is established for each subset with the subset from which it was obtained, so that the links can be interpreted as \"obtained from\", since each column of each subset of data must be associated with a type of experimental data (called a category), especially those corresponding to identifiers that the links are based on. A link is established for each subset with the subset from which it was obtained, so that the links can be interpreted as \"obtained from\", since each column of each subset of data must be associated with a type of experimental data (called a category), especially those corresponding to identifiers that the links are based on. Another good practice is to promote non-proprietary formats such as TSV , which is a necessary and indispensable step towards \"open linked data\" ( 5 gold stars principle). So an ODAM dataset is a bundle that contains a set of TSV files. The TSV files are simple tables containing the data of the dataset. All steps concerning the collection and preparation of data have been published and are available online at protocols.io Data Preparation Protocol for ODAM Compliance. protocols.io https://dx.doi.org/10.17504/protocols.io.betcjeiw The purpose of this protocol is to describe all the steps involved in collecting, preparing and annotating the data from an experiment associated with an experimental design (DoE) that will then allow the user to benefit from the services offered by ODAM. The overall approach is based on good data management practices concerning data structuring and the description of structural metadata. Indeed, ODAM allows to put metadata in depth, i.e. at the level of the data itself (i.e. metadata at column-level such as factors, variables,...) and not only as a \"hat\" on the data set. Thus, having unambiguous (standard) definitions of all internal elements (e.g. column definitions, units of measurement), makes the dataset human and machine readable i.e. with a higher level of interoperability and makes functional interlinking and analysis in broader context much easier. A factor of an experiment is a controlled independent variable; a variable whose levels are set by the experimenter. Treatments (control vs. stress), genotype (WT vs. mutant), the course of time (development stages) or even tissues, are typical factors of experiments. \u21a9 Identifiers precisely reference within the experiment each of the elements belonging to the same observation entity forming a coherent observation unit. F0or example, each plant, each sample has its own identifier, \u21a9 Quantitative data are values that describe a measurable quantity, in the form of numbers that can be calculated. \u21a9 Qualitative data describe qualities or characteristics. They answer questions such as \"what type\" or \"what category\". These values are no longer numbers, but a set of modalities. These values cannot be calculated. \u21a9","title":"Data Type"},{"location":"data_type/#odam-deployment-and-users-guide","text":"","title":"ODAM: Deployment and User's Guide"},{"location":"data_type/#data-collection-and-preparation","text":"","title":"Data collection and preparation"},{"location":"data_type/#data-type","text":"Whatever the kind of experiment, this assumes a design of experiment ( DoE ) involving individuals, samples or whatever things, as the main objects of study and producing several experimental data tables . This also assumes the observation of dependent variables resulting from effects of some controlled independent variables ( factors 1 ). Moreover, the objects of study usually have an identifier 2 for each of them, and the variables can be quantitative 3 or qualitative 4 . Example of an experiment data table viewed according to the repartition by category as introduced in the text","title":"Data Type"},{"location":"data_type/#data-management-promote-good-practices","text":"First and foremost, it is important to have well-organized data. The files generated during data collection have to be organized according to the entity-attribute relational model. Indeed, each entity corresponds to a type of collected data (samples, compounds, ...) for which is associated a set of attributes, i.e. observed or measured variables. Well organized data means that each variable forms a column, each observation forms a line, and each type of \"unit observational\" forms a table, i.e a file. Then, a link is established for each subset with the subset from which it was obtained, so that the links can be interpreted as \"obtained from\", since each column of each subset of data must be associated with a type of experimental data (called a category), especially those corresponding to identifiers that the links are based on. A link is established for each subset with the subset from which it was obtained, so that the links can be interpreted as \"obtained from\", since each column of each subset of data must be associated with a type of experimental data (called a category), especially those corresponding to identifiers that the links are based on. Another good practice is to promote non-proprietary formats such as TSV , which is a necessary and indispensable step towards \"open linked data\" ( 5 gold stars principle). So an ODAM dataset is a bundle that contains a set of TSV files. The TSV files are simple tables containing the data of the dataset. All steps concerning the collection and preparation of data have been published and are available online at protocols.io Data Preparation Protocol for ODAM Compliance. protocols.io https://dx.doi.org/10.17504/protocols.io.betcjeiw The purpose of this protocol is to describe all the steps involved in collecting, preparing and annotating the data from an experiment associated with an experimental design (DoE) that will then allow the user to benefit from the services offered by ODAM. The overall approach is based on good data management practices concerning data structuring and the description of structural metadata. Indeed, ODAM allows to put metadata in depth, i.e. at the level of the data itself (i.e. metadata at column-level such as factors, variables,...) and not only as a \"hat\" on the data set. Thus, having unambiguous (standard) definitions of all internal elements (e.g. column definitions, units of measurement), makes the dataset human and machine readable i.e. with a higher level of interoperability and makes functional interlinking and analysis in broader context much easier. A factor of an experiment is a controlled independent variable; a variable whose levels are set by the experimenter. Treatments (control vs. stress), genotype (WT vs. mutant), the course of time (development stages) or even tissues, are typical factors of experiments. \u21a9 Identifiers precisely reference within the experiment each of the elements belonging to the same observation entity forming a coherent observation unit. F0or example, each plant, each sample has its own identifier, \u21a9 Quantitative data are values that describe a measurable quantity, in the form of numbers that can be calculated. \u21a9 Qualitative data describe qualities or characteristics. They answer questions such as \"what type\" or \"what category\". These values are no longer numbers, but a set of modalities. These values cannot be calculated. \u21a9","title":"Data management : promote good practices"},{"location":"dataset_example/","text":"ODAM: Deployment and User's Guide \u00b6 A dataset example \u00b6 Fruit Integrative Modelling, an ERASysBio+ project : Yves Gibon (Coordinator) The project aimed to build a virtual tomato fruit that enables the prediction of metabolite levels given genetic and environmental inputs, by an iterative process between laboratories which combine expertise in fruit biology, ecophysiology, theoretical and experimental biochemistry, and biotechnology. To build a kinetic model encompassing the routes carbon takes, once imported into the fruit cells from the source organs of the mother plant. To integrate the kinetic model with a phenomenological model predicting sugar and organic acid contents as functions of time, light intensity, temperature and water availability. To obtain large-scale experimental measures of the consequences of altered environmental conditions. To assess the influence of the environment on fruit metabolism, tomato ( Solanum lycopersicum 'Moneymaker' ) plants were grown under contrasting conditions (optimal for commercial, shaded production) and locations. Samples were harvested at nine stages of development, and 36 enzyme activities of central metabolism were measured as well as protein, starch, and major metabolites, such as hexoses, sucrose, organic acids, and amino acids. About 580 tomato plants were grown in a greenhouse in the southwest of France ( Sainte-Livrade sur Lot ) during the summer of 2010 according to usual production practices. Links related to the dataset \u00b6 Description Link Data explorer (*) https://pmb-bordeaux.fr/dataexplorer/?ds=frim1 Dataverse (*) https://doi.org/10.15454/95JUTK Jupyter notebooks (R & Python) https://nbviewer.jupyter.org/github/djacob65/binder_odam/tree/master/ Modeling the growth of tomato fruits based on enzyme activity profiles : An example of data analysis interfaced by ODAM https://hal.inrae.fr/hal-02611223 (*) Both repositories are supported by INRAE (France) for a minimum period of 10 years (until 2030) Publication of the dataset according to FAIR principles \u00b6 Because ODAM is primarily an Experimental Data Table Management System (EDTMS) for data sharing, it must be associated with a suitable data repository in order to support data publishing. So the ODAM approach has to be regarded as complementary with publication of the data online within an institutional data repository as described in re3data.org (e.g. INRAE Data Portal ) associated or not with a scientific paper. To be compliant with the FAIR principles , not all data, documents, workflows and other tools need to be located in a single system, but from a central repository, it is the set of links that constitutes the true information management system. It must be able to be traversed by a human being as well as by machines. Data INRAE repository as a hub (based on Dataverse) allows to interconnect the different elements of the FRIM dataset. By relying on explicit schemas ( JSON-LD , JSON Schema ) for both metadata and data, it becomes possible to reuse the data without friction, both by humans and machines. Indeed an explicit schema allows to define structural metadata along with unambiguous definitions of all internal elements (e.g. column definitions, units of measurement), through links to accessible (standard) definitions. Moreover, this will result in better annotated and more easily usable data that meets effortlessly the FAIR criteria for reusability. Concerning the FAIRification of data, this has a positive impact on the FAIR criteria 'Interoperable' and 'Reusable', encouraging structured data using a discoverable, community-endorsed schema or data model. See also for more details: FAIR grids applied on FRIM dataset ODAM data-package based on JSON-Schema References \u00b6 Biais B, B\u00e9nard C, Beauvoit B, Colombi\u00e9 S, Prodhomme D, M\u00e9nard G, Bernillon S, Gehl B, Gautier H, Ballias P, Mazat J-P, Sweetlove L, G\u00e9nard M, Gibon Y. 2014. Remarkable reproducibility of enzyme activity profiles in tomato fruits grown under contrasting environments provides a roadmap for studies of fruit metabolism. Plant Physiology 164, 1204-1221. doi: 10.1104/pp.113.231241","title":"A dataset example"},{"location":"dataset_example/#odam-deployment-and-users-guide","text":"","title":"ODAM: Deployment and User's Guide"},{"location":"dataset_example/#a-dataset-example","text":"Fruit Integrative Modelling, an ERASysBio+ project : Yves Gibon (Coordinator) The project aimed to build a virtual tomato fruit that enables the prediction of metabolite levels given genetic and environmental inputs, by an iterative process between laboratories which combine expertise in fruit biology, ecophysiology, theoretical and experimental biochemistry, and biotechnology. To build a kinetic model encompassing the routes carbon takes, once imported into the fruit cells from the source organs of the mother plant. To integrate the kinetic model with a phenomenological model predicting sugar and organic acid contents as functions of time, light intensity, temperature and water availability. To obtain large-scale experimental measures of the consequences of altered environmental conditions. To assess the influence of the environment on fruit metabolism, tomato ( Solanum lycopersicum 'Moneymaker' ) plants were grown under contrasting conditions (optimal for commercial, shaded production) and locations. Samples were harvested at nine stages of development, and 36 enzyme activities of central metabolism were measured as well as protein, starch, and major metabolites, such as hexoses, sucrose, organic acids, and amino acids. About 580 tomato plants were grown in a greenhouse in the southwest of France ( Sainte-Livrade sur Lot ) during the summer of 2010 according to usual production practices.","title":"A dataset example"},{"location":"dataset_example/#links-related-to-the-dataset","text":"Description Link Data explorer (*) https://pmb-bordeaux.fr/dataexplorer/?ds=frim1 Dataverse (*) https://doi.org/10.15454/95JUTK Jupyter notebooks (R & Python) https://nbviewer.jupyter.org/github/djacob65/binder_odam/tree/master/ Modeling the growth of tomato fruits based on enzyme activity profiles : An example of data analysis interfaced by ODAM https://hal.inrae.fr/hal-02611223 (*) Both repositories are supported by INRAE (France) for a minimum period of 10 years (until 2030)","title":"Links related to the dataset"},{"location":"dataset_example/#publication-of-the-dataset-according-to-fair-principles","text":"Because ODAM is primarily an Experimental Data Table Management System (EDTMS) for data sharing, it must be associated with a suitable data repository in order to support data publishing. So the ODAM approach has to be regarded as complementary with publication of the data online within an institutional data repository as described in re3data.org (e.g. INRAE Data Portal ) associated or not with a scientific paper. To be compliant with the FAIR principles , not all data, documents, workflows and other tools need to be located in a single system, but from a central repository, it is the set of links that constitutes the true information management system. It must be able to be traversed by a human being as well as by machines. Data INRAE repository as a hub (based on Dataverse) allows to interconnect the different elements of the FRIM dataset. By relying on explicit schemas ( JSON-LD , JSON Schema ) for both metadata and data, it becomes possible to reuse the data without friction, both by humans and machines. Indeed an explicit schema allows to define structural metadata along with unambiguous definitions of all internal elements (e.g. column definitions, units of measurement), through links to accessible (standard) definitions. Moreover, this will result in better annotated and more easily usable data that meets effortlessly the FAIR criteria for reusability. Concerning the FAIRification of data, this has a positive impact on the FAIR criteria 'Interoperable' and 'Reusable', encouraging structured data using a discoverable, community-endorsed schema or data model. See also for more details: FAIR grids applied on FRIM dataset ODAM data-package based on JSON-Schema","title":"Publication of the dataset according to FAIR principles"},{"location":"dataset_example/#references","text":"Biais B, B\u00e9nard C, Beauvoit B, Colombi\u00e9 S, Prodhomme D, M\u00e9nard G, Bernillon S, Gehl B, Gautier H, Ballias P, Mazat J-P, Sweetlove L, G\u00e9nard M, Gibon Y. 2014. Remarkable reproducibility of enzyme activity profiles in tomato fruits grown under contrasting environments provides a roadmap for studies of fruit metabolism. Plant Physiology 164, 1204-1221. doi: 10.1104/pp.113.231241","title":"References"},{"location":"dataverse/","text":"ODAM: Deployment and User's Guide \u00b6 ODAM data-package from a Dataverse repository \u00b6 Example of a session with R showing how it is possible to retrieve a datapackage using a keyword from Data INRAE , a Dataverse repository. ** Introduction to Dataverse with R ** https://cran.r-project.org/web/packages/dataverse/vignettes/A-introduction.html library ( httr ) library ( jsonvalidate ) library ( jsonlite ) library ( dataverse ) # User Parameters dataverse_server <- \"data.inra.fr\" dataset_keyword <- \"frim\" # Get the ODAM data package schema response <- GET ( 'https://inrae.github.io/ODAM/json-schema/odam-data-package.json' , config ( sslversion = 6 , ssl_verifypeer = 1 )) schema <- rawToChar ( response $ content ) # Define the dataverse server as an env. variable Sys.setenv ( \"DATAVERSE_SERVER\" = dataverse_server ) repeat { # Data Search dv <- dataverse_search ( dataset_keyword ) # Check if a dataverse found with a dataset type if ( length ( dv ) == 0 ) break if ( length ( \"dataset\" %in% dv $ type ) == 0 ) break # Get list of files dv <- dv[ dv $ type == \"dataset\" , ] List <- dataset_files ( dv $ global_id ) datafiles <- as.data.frame ( t ( simplify2array ( List ))) # check if a datapackage.json ? if ( length ( \"datapackage.json\" %in% datafiles $ label ) == 0 ) break # Check if it is an ODAM datapackage type id <- which ( \"datapackage.json\" %in% datafiles $ label ) if ( length ( grep ( \"ODAM\" , datafiles $ dataFile[[1]] $ description )) == 0 ) break # Get the datapackage.json content file file_id <- datafiles $ dataFile[[id]] $ id dp_json <- rawToChar ( get_file ( file_id )) if ( is.null ( dp_json )) break # Validate the JSON against the ODAM data package schema if ( ! jsonvalidate :: json_validate ( dp_json , schema ) ) break # Parse the JSON object to a data.frame metadata <- fromJSON ( dp_json ) # Do something with the metadata ... metadata break }","title":"dataverse"},{"location":"dataverse/#odam-deployment-and-users-guide","text":"","title":"ODAM: Deployment and User's Guide"},{"location":"dataverse/#odam-data-package-from-a-dataverse-repository","text":"Example of a session with R showing how it is possible to retrieve a datapackage using a keyword from Data INRAE , a Dataverse repository. ** Introduction to Dataverse with R ** https://cran.r-project.org/web/packages/dataverse/vignettes/A-introduction.html library ( httr ) library ( jsonvalidate ) library ( jsonlite ) library ( dataverse ) # User Parameters dataverse_server <- \"data.inra.fr\" dataset_keyword <- \"frim\" # Get the ODAM data package schema response <- GET ( 'https://inrae.github.io/ODAM/json-schema/odam-data-package.json' , config ( sslversion = 6 , ssl_verifypeer = 1 )) schema <- rawToChar ( response $ content ) # Define the dataverse server as an env. variable Sys.setenv ( \"DATAVERSE_SERVER\" = dataverse_server ) repeat { # Data Search dv <- dataverse_search ( dataset_keyword ) # Check if a dataverse found with a dataset type if ( length ( dv ) == 0 ) break if ( length ( \"dataset\" %in% dv $ type ) == 0 ) break # Get list of files dv <- dv[ dv $ type == \"dataset\" , ] List <- dataset_files ( dv $ global_id ) datafiles <- as.data.frame ( t ( simplify2array ( List ))) # check if a datapackage.json ? if ( length ( \"datapackage.json\" %in% datafiles $ label ) == 0 ) break # Check if it is an ODAM datapackage type id <- which ( \"datapackage.json\" %in% datafiles $ label ) if ( length ( grep ( \"ODAM\" , datafiles $ dataFile[[1]] $ description )) == 0 ) break # Get the datapackage.json content file file_id <- datafiles $ dataFile[[id]] $ id dp_json <- rawToChar ( get_file ( file_id )) if ( is.null ( dp_json )) break # Validate the JSON against the ODAM data package schema if ( ! jsonvalidate :: json_validate ( dp_json , schema ) ) break # Parse the JSON object to a data.frame metadata <- fromJSON ( dp_json ) # Do something with the metadata ... metadata break }","title":"ODAM data-package from a Dataverse repository"},{"location":"getting_started/","text":"ODAM: Deployment and User's Guide \u00b6 ODAM (Open Data for Access and Mining) is an Experiment Data Table Management System (EDTMS) Background \u00b6 In life sciences, (and particularly in plant sciences), each time an experimental design ( DoE ) is implemented, we can, very schematically, represent the data flow according to 4 main steps, from raw data to publication of results. Very Schematically one can represent the data flow according to 4 main steps, from raw data to publication of results. What kind of data From a biological point of view, the data integrating the maximum amount of relevant information are those resulting from the pre-processing of so-called raw data (resulting from analytical techniques) and including annotations with curation then validated by one or more experts; i.e. those involving a transformation of analytical variables (peaks or resonances on spectra, locus on a DNA / RNA / Protein sequence, ...) into biological variables (metabolites, proteins, genes, ...). At this stage, because they are not synthesized, they still have all their variabilities (technological and biological replicas on all factorial levels) and therefore have more potential for reuse. Moreover, these data are not automatically reproducible (e.g. via workflows) because they require human expertise (i.e. know-how). This is why we have focused our data management on these experimental data tables . Data handling When generating data in an experiment involving several types of data from several analytical techniques, and this for the same samples, the task of being able to easily link these different data on the basis of sample identifiers is crucial. This is because the consistency of the data must be ensured throughout the experiment, so that it becomes unnecessary for each member to conduct a laborious investigation to find out who has the correct identifiers. Data sharing Each time we plan to share data coming from a common experimental design, the classical challenges for fast using data by every partner are data storage and data access. We propose an approach for sharing project data all along its development phase, from the setup of the experimental schema up to the data acquisition from the various analyzes of samples, so that all data is readily available as soon as they are generated. Proposed solution \u00b6 ODAM software is designed to manage experimental data tables in a quick and easy way for users. There is no need to develop a complex data model. Just complete the data with some structural metadata. These structural metadata will be used first to make full use of the data as soon as they are produced and formatted and then to annotate the dataset for later dissemination, either to project partners or more widely. The core idea in one shot The central idea which has been the founding idea of ODAM, is that data producers \"just\" have to drag and drop their data tables onto a storage space, which depending on the chosen infrastructure can be local (i.e. their PC, or a NAS) or remote (virtual disk space). So simply dropping data files (along with two additional metadata files) on the storage space allows users to access them through web services. This means there is no need for additional configuration on the server. ODAM proposes to meet certain needs typically encountered during the implementation of an experimental design in life science including several different analyses of the same sample. Data collecting and preparation The formatting of all the data and matching the data from the different analyses with their experimental context can be a long step. Tasks such as collecting and preparing data in order to combine several data sources require a lot of long, repetitive and tedious manipulations. Similarly, when modeling, subsets must be selected and then many scenarios with different parameters must be tested. Data sharing Enabling centralized management of identifiers (e.g. plants, crops, samples, etc.) so that they are unique and shared by all project members. Indeed, as each biological sample is most often aliquoted and then sent for analysis by different techniques, the data returned in tabular form must be able to be linked to the other data according to the identifiers of the samples Giving access to data for rapid use by each project member and this throughout the development phase, from the implementation of the experimental design to the acquisition of data from the various sample analyses, so that all data are readily available as soon as they are generated. Data publishing To be able to publish one's data without a colossal effort of formatting, and without the need for data archaeology. To be able to publish one's data according to the FAIR principles , at least the essentials. To facilitate the reuse of data by providing structural metadata, thus avoiding that data consumers spend a disproportionate amount of time trying to understand the digital resources they need and devising specific ways to combine them. ODAM sofware suite allows experimental data tables to be widely accessible and fully reusable and this with minimal effort on the part of the data provider. Easily structure your data by adding structural metadata so that you can first exploit it locally yourself, before sharing it more widely just as easily. make research data locally or broadly accessible all along the project allow data to be selected then, downloadable by web API allow data and analysis to be visualized online Based on the following criteria: Centrally manage identifiers (plants, harvests, samples, ...) so that they are unique and shared by all Avoid the implementation of a complex data management system (requiring a data model) given that many changes can occur during the project. (possibility of new analysis, new measures or give up some others, ...) Facilitates the subsequent publication of data: either the data can serve to fill in an existing database or the data can be broadcast through a web-service approach with the associated metadata. For this work, we made the choice to keep the good old way of scientist to use worksheets, thus using the same tool for both data files and metadata definition files. The ODAM software can be deployed at multiple scales (local, intranet, internet), depending on the need and the target community. Guideline keywords : simplicity, flexibility, efficiency Give an open access to your data and make them ready to be mined - A data explorer as bonus : Test online with the FRIM dataset Documentation \u00b6 Presentation : A presentation on the ODAM software, its aims and what can we do with it for what purposes. FAIR_and_DataLife_DJ_Oct2019.pdf Presentation : Part 1: How to best manage your data to make the most of it for your research Make your data great now Presentation : Part 2: How to ensure that open data works for research - Towards Linked Data Make your data great again Document : ODAM: Deployment and User's Guide ODAM - Deployment and User's Guide Document : Data Preparation Protocol for ODAM Compliance. protocols.io doi:10.17504/protocols.io.betcjeiw Document :How to install a VM embedding the ODAM software on Oracle VM VirtualBox doi:10.15454/C9LAEF , Portail Data INRAE, V1","title":"Getting started"},{"location":"getting_started/#odam-deployment-and-users-guide","text":"ODAM (Open Data for Access and Mining) is an Experiment Data Table Management System (EDTMS)","title":"ODAM: Deployment and User's Guide"},{"location":"getting_started/#background","text":"In life sciences, (and particularly in plant sciences), each time an experimental design ( DoE ) is implemented, we can, very schematically, represent the data flow according to 4 main steps, from raw data to publication of results. Very Schematically one can represent the data flow according to 4 main steps, from raw data to publication of results. What kind of data From a biological point of view, the data integrating the maximum amount of relevant information are those resulting from the pre-processing of so-called raw data (resulting from analytical techniques) and including annotations with curation then validated by one or more experts; i.e. those involving a transformation of analytical variables (peaks or resonances on spectra, locus on a DNA / RNA / Protein sequence, ...) into biological variables (metabolites, proteins, genes, ...). At this stage, because they are not synthesized, they still have all their variabilities (technological and biological replicas on all factorial levels) and therefore have more potential for reuse. Moreover, these data are not automatically reproducible (e.g. via workflows) because they require human expertise (i.e. know-how). This is why we have focused our data management on these experimental data tables . Data handling When generating data in an experiment involving several types of data from several analytical techniques, and this for the same samples, the task of being able to easily link these different data on the basis of sample identifiers is crucial. This is because the consistency of the data must be ensured throughout the experiment, so that it becomes unnecessary for each member to conduct a laborious investigation to find out who has the correct identifiers. Data sharing Each time we plan to share data coming from a common experimental design, the classical challenges for fast using data by every partner are data storage and data access. We propose an approach for sharing project data all along its development phase, from the setup of the experimental schema up to the data acquisition from the various analyzes of samples, so that all data is readily available as soon as they are generated.","title":"Background"},{"location":"getting_started/#proposed-solution","text":"ODAM software is designed to manage experimental data tables in a quick and easy way for users. There is no need to develop a complex data model. Just complete the data with some structural metadata. These structural metadata will be used first to make full use of the data as soon as they are produced and formatted and then to annotate the dataset for later dissemination, either to project partners or more widely. The core idea in one shot The central idea which has been the founding idea of ODAM, is that data producers \"just\" have to drag and drop their data tables onto a storage space, which depending on the chosen infrastructure can be local (i.e. their PC, or a NAS) or remote (virtual disk space). So simply dropping data files (along with two additional metadata files) on the storage space allows users to access them through web services. This means there is no need for additional configuration on the server. ODAM proposes to meet certain needs typically encountered during the implementation of an experimental design in life science including several different analyses of the same sample. Data collecting and preparation The formatting of all the data and matching the data from the different analyses with their experimental context can be a long step. Tasks such as collecting and preparing data in order to combine several data sources require a lot of long, repetitive and tedious manipulations. Similarly, when modeling, subsets must be selected and then many scenarios with different parameters must be tested. Data sharing Enabling centralized management of identifiers (e.g. plants, crops, samples, etc.) so that they are unique and shared by all project members. Indeed, as each biological sample is most often aliquoted and then sent for analysis by different techniques, the data returned in tabular form must be able to be linked to the other data according to the identifiers of the samples Giving access to data for rapid use by each project member and this throughout the development phase, from the implementation of the experimental design to the acquisition of data from the various sample analyses, so that all data are readily available as soon as they are generated. Data publishing To be able to publish one's data without a colossal effort of formatting, and without the need for data archaeology. To be able to publish one's data according to the FAIR principles , at least the essentials. To facilitate the reuse of data by providing structural metadata, thus avoiding that data consumers spend a disproportionate amount of time trying to understand the digital resources they need and devising specific ways to combine them. ODAM sofware suite allows experimental data tables to be widely accessible and fully reusable and this with minimal effort on the part of the data provider. Easily structure your data by adding structural metadata so that you can first exploit it locally yourself, before sharing it more widely just as easily. make research data locally or broadly accessible all along the project allow data to be selected then, downloadable by web API allow data and analysis to be visualized online Based on the following criteria: Centrally manage identifiers (plants, harvests, samples, ...) so that they are unique and shared by all Avoid the implementation of a complex data management system (requiring a data model) given that many changes can occur during the project. (possibility of new analysis, new measures or give up some others, ...) Facilitates the subsequent publication of data: either the data can serve to fill in an existing database or the data can be broadcast through a web-service approach with the associated metadata. For this work, we made the choice to keep the good old way of scientist to use worksheets, thus using the same tool for both data files and metadata definition files. The ODAM software can be deployed at multiple scales (local, intranet, internet), depending on the need and the target community. Guideline keywords : simplicity, flexibility, efficiency Give an open access to your data and make them ready to be mined - A data explorer as bonus : Test online with the FRIM dataset","title":"Proposed solution"},{"location":"getting_started/#documentation","text":"Presentation : A presentation on the ODAM software, its aims and what can we do with it for what purposes. FAIR_and_DataLife_DJ_Oct2019.pdf Presentation : Part 1: How to best manage your data to make the most of it for your research Make your data great now Presentation : Part 2: How to ensure that open data works for research - Towards Linked Data Make your data great again Document : ODAM: Deployment and User's Guide ODAM - Deployment and User's Guide Document : Data Preparation Protocol for ODAM Compliance. protocols.io doi:10.17504/protocols.io.betcjeiw Document :How to install a VM embedding the ODAM software on Oracle VM VirtualBox doi:10.15454/C9LAEF , Portail Data INRAE, V1","title":"Documentation"},{"location":"json-schema/","text":"ODAM: Deployment and User's Guide \u00b6 ODAM data-package based on JSON-Schema \u00b6 A data package is a simple container format based on JSON Schema specifications used to describe and package a collection of data. Defining an explicit schema for structural metadata allows machines to better interpret the data for reuse. Thus, when disseminating data, a file named datapackage.json by convention can be added to the collection of your data files. This datapackage.json file contains all structural metadata along with unambiguous definitions of all internal elements (e.g. column definitions, units of measurement), through links to accessible (standard) definitions. The datapackage.json file can be generated directly from the ODAM API by specifying ' /datapackage ' at the end of the request. By default, the reference to the data files is relative. To have a URL as reference for the data files, it is necessary to add at the end of the request ' ?links=1 ' ODAM data package schema is very close to the Frictionless Data framework. odam-data-package.json : JSON Schema for ODAM datapackage odam-data-resource.json : JSON Schema for ODAM dataresource ** ODAM data-package on github ** https://github.com/djacob65/odam-datapackage ** Frictionless Data Specifications ** http://specs.frictionlessdata.io/ https://github.com/frictionlessdata/specs/tree/master/schemas Session example with R using a datapackage \u00b6 In this example, the datapackage is directly generated from an ODAM repository . But it is also possible to retrieve it from a Dataverse repository from a keyword. Cf Example ** json_validate: Validate a json file ** https://rdrr.io/cran/jsonvalidate/man/json_validate.html Code library ( httr ) library ( jsonvalidate ) library ( jsonlite ) options ( width = 256 ) # URL of the ODAM data repository odam_url <- 'https://pmb-bordeaux.fr' # ID of the dataset dataset <- 'frim1' # Get the ODAM data package schema response <- GET ( 'https://inrae.github.io/ODAM/json-schema/odam-data-package.json' , config ( sslversion = 6 , ssl_verifypeer = 1 )) schema <- rawToChar ( response $ content ) # Get structural metadata information in datapackage format (json) # for the 'frim1' dataset directly from an ODAM repository # (located on https://pmb-bordeaux.fr) # As the option links is set to 1, we will have the absolute reference # for data files (see below) response <- GET ( sprintf ( '%s/getdata/json/%s/datapackage?links=1' , odam_url , dataset ), config ( sslversion = 6 , ssl_verifypeer = 1 )) dp_json <- rawToChar ( response $ content ) # Validate the JSON against the ODAM data package schema jsonvalidate :: json_validate ( dp_json , schema ) Output [1] TRUE Code # Parse the JSON object to a data.frame dp <- fromJSON ( dp_json ) resources <- dp $ resources # List some metadata about the dataset resources[ c ( \"name\" , \"title\" , \"identifier\" , \"obtainedFrom\" , \"joinkey\" , \"path\" ) ] Output name title identifier obtainedFrom joinkey path 1 plants Plant features PlantID < NA > < NA > https : //pmb-bordeaux.fr/getdata/tsv/frim1/plants 2 samples Sample features SampleID plants PlantID https : //pmb-bordeaux.fr/getdata/tsv/frim1/samples 3 aliquots Aliquots features AliquotID samples SampleID https : //pmb-bordeaux.fr/getdata/tsv/frim1/aliquots 4 cellwall_metabo Cell wall Compound quantifications AliquotID aliquots AliquotID https : //pmb-bordeaux.fr/getdata/tsv/frim1/cellwall_metabo 5 cellwall_metaboFW Cell Wall Compound quantifications ( FW ) AliquotID aliquots AliquotID https : //pmb-bordeaux.fr/getdata/tsv/frim1/cellwall_metaboFW 6 activome Activome Features AliquotID aliquots AliquotID https : //pmb-bordeaux.fr/getdata/tsv/frim1/activome 7 pools Pools of remaining pools PoolID samples SampleID https : //pmb-bordeaux.fr/getdata/tsv/frim1/pools 8 qMS_metabo MS Compounds quantification PoolID pools PoolID https : //pmb-bordeaux.fr/getdata/tsv/frim1/qMS_metabo 9 qNMR_metabo NMR Compounds quantification PoolID pools PoolID https : //pmb-bordeaux.fr/getdata/tsv/frim1/qNMR_metabo 10 plato_hexosesP Hexoses Phosphate AliquotID aliquots AliquotID https : //pmb-bordeaux.fr/getdata/tsv/frim1/plato_hexosesP 11 lipids_AG Lipids AG AliquotID aliquots AliquotID https : //pmb-bordeaux.fr/getdata/tsv/frim1/lipids_AG 12 AminoAcid Amino Acids AliquotID aliquots AliquotID https : //pmb-bordeaux.fr/getdata/tsv/frim1/AminoAcid Code # Read the 'samples' data file - index=2 index <- 2 M <- read.table ( url ( resources[ \"path\" ] $ path[index] ), header = resources $ dialect $ header[index] , sep = resources $ dialect $ delimiter[index] ) # Display an extract M[1 : 10 , ] Output SampleID PlantID Truss DevStage FruitAge HarvestDate HarvestHour FruitPosition FruitDiameter FruitHeight FruitFW FruitDW DW 1 1 A26 T5 FF .01 08 DPA 40379 0.5 2 NA NA 0.72 0.090216 NA 2 1 C2 T5 FF .01 08 DPA 40379 0.5 3 NA NA 0.56 0.070168 NA 3 1 D15 T5 FF .01 08 DPA 40379 0.5 4 NA NA 0.78 0.097734 NA 4 1 E19 T5 FF .01 08 DPA 40379 0.5 4 NA NA 0.66 0.082698 NA 5 1 E34 T5 FF .01 08 DPA 40379 0.5 3 NA NA 0.7 0.087710 NA 6 1 E38 T5 FF .01 08 DPA 40379 0.5 3 NA NA 0.7 0.087710 NA 7 1 H29 T5 FF .01 08 DPA 40379 0.5 5 NA NA 1.24 0.155372 NA 8 1 H34 T5 FF .01 08 DPA 40379 0.5 4 NA NA 0.86 0.107758 NA 9 1 H52 T5 FF .01 08 DPA 40379 0.5 5 NA NA 0.77 0.096481 NA 10 1 H61 T5 FF .01 08 DPA 40379 0.5 5 NA NA 0.56 0.070168 NA Code # Get the categories for the 'samples' data subset categories <- resources $ schema $ categories[[index]] categories # List the 'quantitative' variables for the 'samples' data subset categories $ fields[ categories $ name == \"quantitative\" ][[1]] Output name fields 1 identifier SampleID 2 factor DevStage , FruitAge 3 quantitative FruitDiameter , FruitHeight , FruitFW , FruitDW , DW 4 qualitative Truss [ 1 ] \"FruitDiameter\" \"FruitHeight\" \"FruitFW\" \"FruitDW\" \"DW\" Code # Get the list of data subsets that were obtained from the 'samples' index <- 2 subsets <- resources $ schema $ foreignKey[[index]] $ reference[ , 1 ] if ( ! is.null ( subsets ) ) resources[ resources $ name %in% subsets , c ( \"name\" , \"title\" , \"identifier\" , \"obtainedFrom\" ) ] # Get the list of data subsets that were obtained from the 'aliquots' index <- 3 subsets <- resources $ schema $ foreignKey[[index]] $ reference[ , 1 ] if ( ! is.null ( subsets ) ) resources[ resources $ name %in% subsets , c ( \"name\" , \"title\" , \"identifier\" , \"obtainedFrom\" ) ] Output # List of data subsets that were obtained from the 'samples' name title identifier obtainedFrom 3 aliquots Aliquots features AliquotID samples 7 pools Pools of remaining pools PoolID samples # List of data subsets that were obtained from the 'aliquots' name title identifier obtainedFrom 4 cellwall_metabo Cell wall Compound quantifications AliquotID aliquots 5 cellwall_metaboFW Cell Wall Compound quantifications ( FW ) AliquotID aliquots 6 activome Activome Features AliquotID aliquots 10 plato_hexosesP Hexoses Phosphate AliquotID aliquots 11 lipids_AG Lipids AG AliquotID aliquots 12 AminoAcid Amino Acids AliquotID aliquots Code # getMergedDataset : # Returns the data subset resulting from the merging of each data subset from which # the previous data subset was obtained (i.e. going back up the chain of linking # data subsets) # Method : Performs a 'Rigth Join' between the different tables (data subsets) getMergedDataset <- function ( resources , subset , verbose = FALSE ) { M <- NULL s <- subset while ( sum ( resources $ name %in% s ) ) { if ( verbose ) cat ( \"data subset \" , s , \"\\n\" ) # index for subset s i1 <- which ( resources $ name %in% s ) # get metadata from subset s m1 <- resources[i1 , ] # Read data for subset s M1 <- read.table ( url ( resources[ \"path\" ] $ path[i1] ), header = resources $ dialect $ header[i1] , sep = resources $ dialect $ delimiter[i1] ) if ( ! is.null ( M )) # Merge the subset s - Right Join M <- merge ( M1 , M , by = m1 $ identifier , all.y = TRUE ) else M <- M1 # the data subset from which it was obtained. s <- m1 $ obtainedFrom # End of the chain ? if ( is.na ( s )) break } unique ( M ) } # Merge the 'plants', 'samples', 'aliquots' and 'activome' data subsets M <- getMergedDataset ( resources , 'activome' , TRUE ) dim ( M ) # Display column names of the merged data subset colnames ( M ) Output data subset activome data subset aliquots data subset samples data subset plants [ 1 ] 1272 55 [ 1 ] \"PlantID\" \"Rank\" \"PlantNum\" \"Treatment\" \"SampleID\" [ 6 ] \"Truss\" \"DevStage\" \"FruitAge\" \"HarvestDate\" \"HarvestHour\" [ 11 ] \"FruitPosition\" \"FruitDiameter\" \"FruitHeight\" \"FruitFW\" \"FruitDW\" [ 16 ] \"DW\" \"AliquotID\" \"PGM\" \"cFBPase\" \"PyrK\" [ 21 ] \"CitS\" \"PFP\" \"Aconitase\" \"PFK\" \"FruK\" [ 26 ] \"pFBPase\" \"GluK\" \"NAD_ISODH\" \"Enolase\" \"NADP_ISODH\" [ 31 ] \"PEPC\" \"Aldolase\" \"Succ_CoA_ligase\" \"NAD_MalDH\" \"AlaAT\" [ 36 ] \"Fumarase\" \"AspAT\" \"NADP_GluDH\" \"NAD_GAPDH\" \"NADP_GAPDH\" [ 41 ] \"NAD_GluDH\" \"TPI\" \"PGK\" \"Neutral_Inv\" \"Acid_Inv\" [ 46 ] \"G6PDH\" \"UGPase\" \"SuSy\" \"NAD_ME\" \"ShiDH\" [ 51 ] \"NADP_ME\" \"PGI\" \"StarchS\" \"AGPase\" \"SPS\" Session example with Python using a datapackage \u00b6 ** datapackage-py ** https://pypi.org/project/datapackage/ Code import pandas as pd from datapackage import Package , Resource url = 'https://pmb-bordeaux.fr/getdata/json/frim1/datapackage?links=1' dp = Package ( url ) dp . resource_names Output ['plants', 'samples', 'aliquots', 'cellwall_metabo', 'cellwall_metaboFW', 'activome', 'pools', 'qMS_metabo', 'qNMR_metabo', 'plato_hexosesP', 'lipids_AG', 'AminoAcid'] Code # Get the 'plants' subsets into a data.frame id = dp . resource_names . index ( 'plants' ) df = pd . DataFrame ( dp . resources [ id ] . read ( keyed = True )) # Print the data.frame df Output PlantID Rank PlantNum Treatment 0 A1 A 1 Control 1 A2 A 2 Control 2 A3 A 3 Control 3 A4 A 4 Control 4 A5 A 5 Control .. ... ... ... ... 547 G65 G 548 WaterStress 548 G66 G 549 WaterStress 549 G67 G 550 WaterStress 550 G68 G 551 WaterStress 551 G69 G 552 WaterStress [552 rows x 4 columns] Code # Get the headers of the first resource (id=0) dp . resources [ 0 ] . _Resource__table_options Output {'scheme': None, 'format': 'csv', 'encoding': 'utf-8', 'pick_fields': None, 'skip_fields': None, 'pick_rows': [], 'skip_rows': [], 'delimiter': '\\t', 'doublequote': False, 'lineterminator': '\\n', 'skipinitialspace': True} Code # Get the resource descriptor of the 'samples' id = dp . resource_names . index ( 'samples' ) res = dp . resources [ id ] . _Resource__current_descriptor # List some attributes for x in [ 'path' , 'profile' , 'name' , 'title' , 'identifier' , 'obtainedFrom' , 'joinkey' ]: print ( \" %s : %s \" % ( x , res [ x ])) Output path: https://pmb-bordeaux.fr/getdata/tsv/frim1/samples profile: https://inrae.github.io/ODAM/json-schema/odam-data-resource.json name: samples title: Sample features identifier: SampleID obtainedFrom: plants joinkey: PlantID Code # Get the categories of the 'samples' pd . DataFrame ( res [ 'schema' ][ 'categories' ]) Output name fields 0 identifier [SampleID] 1 factor [DevStage, FruitAge] 2 quantitative [FruitDiameter, FruitHeight, FruitFW, FruitDW,... 3 qualitative [Truss] Code # Get the data subsets that were obtained from the 'samples' print ( pd . DataFrame ( res [ 'schema' ][ 'foreignKey' ])) the_list = [] for x in res [ 'schema' ][ 'foreignKey' ]: the_list += [ x [ 'reference' ][ 'resource' ] ] the_list Output fields reference 0 SampleID {'resource': 'aliquots', 'fields': 'SampleID'} 1 SampleID {'resource': 'pools', 'fields': 'SampleID'} ['aliquots', 'pools'] Code # Merge the 'samples' subset with the data subset from which it was obtained id = dp . resource_names . index ( 'samples' ) res = dp . resources [ id ] . _Resource__current_descriptor id_from = dp . resource_names . index ( res [ 'obtainedFrom' ]) df_from = pd . DataFrame ( dp . resources [ id_from ] . read ( keyed = True )) df = pd . DataFrame ( dp . resources [ id ] . read ( keyed = True )) # Right join merged_inner = pd . merge ( left = df_from , right = df , how = 'right' , left_on = res [ 'joinkey' ], right_on = res [ 'joinkey' ]) merged_inner Output PlantID Rank PlantNum Treatment ... FruitHeight FruitFW FruitDW DW 0 A1 A 1 Control ... 10.42 0.81 0.098091 None 1 A1 A 1 Control ... 31.77 21.43 1.470098 None 2 A1 A 1 Control ... 46.85 64.05 4.18887 None 3 A1 A 1 Control ... 43.35 66.64 3.338664 None 4 A2 A 2 Control ... 44.93 66.98 3.355698 None ... ... ... ... ... ... ... ... ... 1282 G67 G 550 WaterStress ... 45.72 70.23 None None 1283 G68 G 551 WaterStress ... 36.56 38.6 None None 1284 G69 G 552 WaterStress ... 39.1 45.47 2.63726 None 1285 G69 G 552 WaterStress ... 46.59 65.73 3.227343 None 1286 G69 G 552 WaterStress ... 40.4 58.51 4.908989 None [1287 rows x 16 columns]","title":"json-schema"},{"location":"json-schema/#odam-deployment-and-users-guide","text":"","title":"ODAM: Deployment and User's Guide"},{"location":"json-schema/#odam-data-package-based-on-json-schema","text":"A data package is a simple container format based on JSON Schema specifications used to describe and package a collection of data. Defining an explicit schema for structural metadata allows machines to better interpret the data for reuse. Thus, when disseminating data, a file named datapackage.json by convention can be added to the collection of your data files. This datapackage.json file contains all structural metadata along with unambiguous definitions of all internal elements (e.g. column definitions, units of measurement), through links to accessible (standard) definitions. The datapackage.json file can be generated directly from the ODAM API by specifying ' /datapackage ' at the end of the request. By default, the reference to the data files is relative. To have a URL as reference for the data files, it is necessary to add at the end of the request ' ?links=1 ' ODAM data package schema is very close to the Frictionless Data framework. odam-data-package.json : JSON Schema for ODAM datapackage odam-data-resource.json : JSON Schema for ODAM dataresource ** ODAM data-package on github ** https://github.com/djacob65/odam-datapackage ** Frictionless Data Specifications ** http://specs.frictionlessdata.io/ https://github.com/frictionlessdata/specs/tree/master/schemas","title":"ODAM data-package based on JSON-Schema"},{"location":"json-schema/#session-example-with-r-using-a-datapackage","text":"In this example, the datapackage is directly generated from an ODAM repository . But it is also possible to retrieve it from a Dataverse repository from a keyword. Cf Example ** json_validate: Validate a json file ** https://rdrr.io/cran/jsonvalidate/man/json_validate.html Code library ( httr ) library ( jsonvalidate ) library ( jsonlite ) options ( width = 256 ) # URL of the ODAM data repository odam_url <- 'https://pmb-bordeaux.fr' # ID of the dataset dataset <- 'frim1' # Get the ODAM data package schema response <- GET ( 'https://inrae.github.io/ODAM/json-schema/odam-data-package.json' , config ( sslversion = 6 , ssl_verifypeer = 1 )) schema <- rawToChar ( response $ content ) # Get structural metadata information in datapackage format (json) # for the 'frim1' dataset directly from an ODAM repository # (located on https://pmb-bordeaux.fr) # As the option links is set to 1, we will have the absolute reference # for data files (see below) response <- GET ( sprintf ( '%s/getdata/json/%s/datapackage?links=1' , odam_url , dataset ), config ( sslversion = 6 , ssl_verifypeer = 1 )) dp_json <- rawToChar ( response $ content ) # Validate the JSON against the ODAM data package schema jsonvalidate :: json_validate ( dp_json , schema ) Output [1] TRUE Code # Parse the JSON object to a data.frame dp <- fromJSON ( dp_json ) resources <- dp $ resources # List some metadata about the dataset resources[ c ( \"name\" , \"title\" , \"identifier\" , \"obtainedFrom\" , \"joinkey\" , \"path\" ) ] Output name title identifier obtainedFrom joinkey path 1 plants Plant features PlantID < NA > < NA > https : //pmb-bordeaux.fr/getdata/tsv/frim1/plants 2 samples Sample features SampleID plants PlantID https : //pmb-bordeaux.fr/getdata/tsv/frim1/samples 3 aliquots Aliquots features AliquotID samples SampleID https : //pmb-bordeaux.fr/getdata/tsv/frim1/aliquots 4 cellwall_metabo Cell wall Compound quantifications AliquotID aliquots AliquotID https : //pmb-bordeaux.fr/getdata/tsv/frim1/cellwall_metabo 5 cellwall_metaboFW Cell Wall Compound quantifications ( FW ) AliquotID aliquots AliquotID https : //pmb-bordeaux.fr/getdata/tsv/frim1/cellwall_metaboFW 6 activome Activome Features AliquotID aliquots AliquotID https : //pmb-bordeaux.fr/getdata/tsv/frim1/activome 7 pools Pools of remaining pools PoolID samples SampleID https : //pmb-bordeaux.fr/getdata/tsv/frim1/pools 8 qMS_metabo MS Compounds quantification PoolID pools PoolID https : //pmb-bordeaux.fr/getdata/tsv/frim1/qMS_metabo 9 qNMR_metabo NMR Compounds quantification PoolID pools PoolID https : //pmb-bordeaux.fr/getdata/tsv/frim1/qNMR_metabo 10 plato_hexosesP Hexoses Phosphate AliquotID aliquots AliquotID https : //pmb-bordeaux.fr/getdata/tsv/frim1/plato_hexosesP 11 lipids_AG Lipids AG AliquotID aliquots AliquotID https : //pmb-bordeaux.fr/getdata/tsv/frim1/lipids_AG 12 AminoAcid Amino Acids AliquotID aliquots AliquotID https : //pmb-bordeaux.fr/getdata/tsv/frim1/AminoAcid Code # Read the 'samples' data file - index=2 index <- 2 M <- read.table ( url ( resources[ \"path\" ] $ path[index] ), header = resources $ dialect $ header[index] , sep = resources $ dialect $ delimiter[index] ) # Display an extract M[1 : 10 , ] Output SampleID PlantID Truss DevStage FruitAge HarvestDate HarvestHour FruitPosition FruitDiameter FruitHeight FruitFW FruitDW DW 1 1 A26 T5 FF .01 08 DPA 40379 0.5 2 NA NA 0.72 0.090216 NA 2 1 C2 T5 FF .01 08 DPA 40379 0.5 3 NA NA 0.56 0.070168 NA 3 1 D15 T5 FF .01 08 DPA 40379 0.5 4 NA NA 0.78 0.097734 NA 4 1 E19 T5 FF .01 08 DPA 40379 0.5 4 NA NA 0.66 0.082698 NA 5 1 E34 T5 FF .01 08 DPA 40379 0.5 3 NA NA 0.7 0.087710 NA 6 1 E38 T5 FF .01 08 DPA 40379 0.5 3 NA NA 0.7 0.087710 NA 7 1 H29 T5 FF .01 08 DPA 40379 0.5 5 NA NA 1.24 0.155372 NA 8 1 H34 T5 FF .01 08 DPA 40379 0.5 4 NA NA 0.86 0.107758 NA 9 1 H52 T5 FF .01 08 DPA 40379 0.5 5 NA NA 0.77 0.096481 NA 10 1 H61 T5 FF .01 08 DPA 40379 0.5 5 NA NA 0.56 0.070168 NA Code # Get the categories for the 'samples' data subset categories <- resources $ schema $ categories[[index]] categories # List the 'quantitative' variables for the 'samples' data subset categories $ fields[ categories $ name == \"quantitative\" ][[1]] Output name fields 1 identifier SampleID 2 factor DevStage , FruitAge 3 quantitative FruitDiameter , FruitHeight , FruitFW , FruitDW , DW 4 qualitative Truss [ 1 ] \"FruitDiameter\" \"FruitHeight\" \"FruitFW\" \"FruitDW\" \"DW\" Code # Get the list of data subsets that were obtained from the 'samples' index <- 2 subsets <- resources $ schema $ foreignKey[[index]] $ reference[ , 1 ] if ( ! is.null ( subsets ) ) resources[ resources $ name %in% subsets , c ( \"name\" , \"title\" , \"identifier\" , \"obtainedFrom\" ) ] # Get the list of data subsets that were obtained from the 'aliquots' index <- 3 subsets <- resources $ schema $ foreignKey[[index]] $ reference[ , 1 ] if ( ! is.null ( subsets ) ) resources[ resources $ name %in% subsets , c ( \"name\" , \"title\" , \"identifier\" , \"obtainedFrom\" ) ] Output # List of data subsets that were obtained from the 'samples' name title identifier obtainedFrom 3 aliquots Aliquots features AliquotID samples 7 pools Pools of remaining pools PoolID samples # List of data subsets that were obtained from the 'aliquots' name title identifier obtainedFrom 4 cellwall_metabo Cell wall Compound quantifications AliquotID aliquots 5 cellwall_metaboFW Cell Wall Compound quantifications ( FW ) AliquotID aliquots 6 activome Activome Features AliquotID aliquots 10 plato_hexosesP Hexoses Phosphate AliquotID aliquots 11 lipids_AG Lipids AG AliquotID aliquots 12 AminoAcid Amino Acids AliquotID aliquots Code # getMergedDataset : # Returns the data subset resulting from the merging of each data subset from which # the previous data subset was obtained (i.e. going back up the chain of linking # data subsets) # Method : Performs a 'Rigth Join' between the different tables (data subsets) getMergedDataset <- function ( resources , subset , verbose = FALSE ) { M <- NULL s <- subset while ( sum ( resources $ name %in% s ) ) { if ( verbose ) cat ( \"data subset \" , s , \"\\n\" ) # index for subset s i1 <- which ( resources $ name %in% s ) # get metadata from subset s m1 <- resources[i1 , ] # Read data for subset s M1 <- read.table ( url ( resources[ \"path\" ] $ path[i1] ), header = resources $ dialect $ header[i1] , sep = resources $ dialect $ delimiter[i1] ) if ( ! is.null ( M )) # Merge the subset s - Right Join M <- merge ( M1 , M , by = m1 $ identifier , all.y = TRUE ) else M <- M1 # the data subset from which it was obtained. s <- m1 $ obtainedFrom # End of the chain ? if ( is.na ( s )) break } unique ( M ) } # Merge the 'plants', 'samples', 'aliquots' and 'activome' data subsets M <- getMergedDataset ( resources , 'activome' , TRUE ) dim ( M ) # Display column names of the merged data subset colnames ( M ) Output data subset activome data subset aliquots data subset samples data subset plants [ 1 ] 1272 55 [ 1 ] \"PlantID\" \"Rank\" \"PlantNum\" \"Treatment\" \"SampleID\" [ 6 ] \"Truss\" \"DevStage\" \"FruitAge\" \"HarvestDate\" \"HarvestHour\" [ 11 ] \"FruitPosition\" \"FruitDiameter\" \"FruitHeight\" \"FruitFW\" \"FruitDW\" [ 16 ] \"DW\" \"AliquotID\" \"PGM\" \"cFBPase\" \"PyrK\" [ 21 ] \"CitS\" \"PFP\" \"Aconitase\" \"PFK\" \"FruK\" [ 26 ] \"pFBPase\" \"GluK\" \"NAD_ISODH\" \"Enolase\" \"NADP_ISODH\" [ 31 ] \"PEPC\" \"Aldolase\" \"Succ_CoA_ligase\" \"NAD_MalDH\" \"AlaAT\" [ 36 ] \"Fumarase\" \"AspAT\" \"NADP_GluDH\" \"NAD_GAPDH\" \"NADP_GAPDH\" [ 41 ] \"NAD_GluDH\" \"TPI\" \"PGK\" \"Neutral_Inv\" \"Acid_Inv\" [ 46 ] \"G6PDH\" \"UGPase\" \"SuSy\" \"NAD_ME\" \"ShiDH\" [ 51 ] \"NADP_ME\" \"PGI\" \"StarchS\" \"AGPase\" \"SPS\"","title":"Session example with R using a datapackage"},{"location":"json-schema/#session-example-with-python-using-a-datapackage","text":"** datapackage-py ** https://pypi.org/project/datapackage/ Code import pandas as pd from datapackage import Package , Resource url = 'https://pmb-bordeaux.fr/getdata/json/frim1/datapackage?links=1' dp = Package ( url ) dp . resource_names Output ['plants', 'samples', 'aliquots', 'cellwall_metabo', 'cellwall_metaboFW', 'activome', 'pools', 'qMS_metabo', 'qNMR_metabo', 'plato_hexosesP', 'lipids_AG', 'AminoAcid'] Code # Get the 'plants' subsets into a data.frame id = dp . resource_names . index ( 'plants' ) df = pd . DataFrame ( dp . resources [ id ] . read ( keyed = True )) # Print the data.frame df Output PlantID Rank PlantNum Treatment 0 A1 A 1 Control 1 A2 A 2 Control 2 A3 A 3 Control 3 A4 A 4 Control 4 A5 A 5 Control .. ... ... ... ... 547 G65 G 548 WaterStress 548 G66 G 549 WaterStress 549 G67 G 550 WaterStress 550 G68 G 551 WaterStress 551 G69 G 552 WaterStress [552 rows x 4 columns] Code # Get the headers of the first resource (id=0) dp . resources [ 0 ] . _Resource__table_options Output {'scheme': None, 'format': 'csv', 'encoding': 'utf-8', 'pick_fields': None, 'skip_fields': None, 'pick_rows': [], 'skip_rows': [], 'delimiter': '\\t', 'doublequote': False, 'lineterminator': '\\n', 'skipinitialspace': True} Code # Get the resource descriptor of the 'samples' id = dp . resource_names . index ( 'samples' ) res = dp . resources [ id ] . _Resource__current_descriptor # List some attributes for x in [ 'path' , 'profile' , 'name' , 'title' , 'identifier' , 'obtainedFrom' , 'joinkey' ]: print ( \" %s : %s \" % ( x , res [ x ])) Output path: https://pmb-bordeaux.fr/getdata/tsv/frim1/samples profile: https://inrae.github.io/ODAM/json-schema/odam-data-resource.json name: samples title: Sample features identifier: SampleID obtainedFrom: plants joinkey: PlantID Code # Get the categories of the 'samples' pd . DataFrame ( res [ 'schema' ][ 'categories' ]) Output name fields 0 identifier [SampleID] 1 factor [DevStage, FruitAge] 2 quantitative [FruitDiameter, FruitHeight, FruitFW, FruitDW,... 3 qualitative [Truss] Code # Get the data subsets that were obtained from the 'samples' print ( pd . DataFrame ( res [ 'schema' ][ 'foreignKey' ])) the_list = [] for x in res [ 'schema' ][ 'foreignKey' ]: the_list += [ x [ 'reference' ][ 'resource' ] ] the_list Output fields reference 0 SampleID {'resource': 'aliquots', 'fields': 'SampleID'} 1 SampleID {'resource': 'pools', 'fields': 'SampleID'} ['aliquots', 'pools'] Code # Merge the 'samples' subset with the data subset from which it was obtained id = dp . resource_names . index ( 'samples' ) res = dp . resources [ id ] . _Resource__current_descriptor id_from = dp . resource_names . index ( res [ 'obtainedFrom' ]) df_from = pd . DataFrame ( dp . resources [ id_from ] . read ( keyed = True )) df = pd . DataFrame ( dp . resources [ id ] . read ( keyed = True )) # Right join merged_inner = pd . merge ( left = df_from , right = df , how = 'right' , left_on = res [ 'joinkey' ], right_on = res [ 'joinkey' ]) merged_inner Output PlantID Rank PlantNum Treatment ... FruitHeight FruitFW FruitDW DW 0 A1 A 1 Control ... 10.42 0.81 0.098091 None 1 A1 A 1 Control ... 31.77 21.43 1.470098 None 2 A1 A 1 Control ... 46.85 64.05 4.18887 None 3 A1 A 1 Control ... 43.35 66.64 3.338664 None 4 A2 A 2 Control ... 44.93 66.98 3.355698 None ... ... ... ... ... ... ... ... ... 1282 G67 G 550 WaterStress ... 45.72 70.23 None None 1283 G68 G 551 WaterStress ... 36.56 38.6 None None 1284 G69 G 552 WaterStress ... 39.1 45.47 2.63726 None 1285 G69 G 552 WaterStress ... 46.59 65.73 3.227343 None 1286 G69 G 552 WaterStress ... 40.4 58.51 4.908989 None [1287 rows x 16 columns]","title":"Session example with Python using a datapackage"},{"location":"tests/","text":"ODAM: Deployment and User's Guide \u00b6 First Header Second Header Third Header Left Center Right Left Center Right Data Preparation Protocol for ODAM Compliance. protocols.io https://dx.doi.org/10.17504/protocols.io.betcjeiw Data Preparation Protocol for ODAM Compliance. protocols.io https://dx.doi.org/10.17504/protocols.io.betcjeiw Tab 1 Markdown content . Tab 2 More Markdown content . list item a list item b Tab 3 More content.","title":"Tests"},{"location":"tests/#odam-deployment-and-users-guide","text":"First Header Second Header Third Header Left Center Right Left Center Right Data Preparation Protocol for ODAM Compliance. protocols.io https://dx.doi.org/10.17504/protocols.io.betcjeiw Data Preparation Protocol for ODAM Compliance. protocols.io https://dx.doi.org/10.17504/protocols.io.betcjeiw Tab 1 Markdown content . Tab 2 More Markdown content . list item a list item b Tab 3 More content.","title":"ODAM: Deployment and User's Guide"},{"location":"fair/fair-frim/","text":"ODAM: Deployment and User's Guide \u00b6 FAIR grids applied on FRIM dataset \u00b6 Three FAIR grids avery different from each other. 1- 5 \u2605 Data Rating Tool From OZONOME , it aims to carry out an evaluation based on the FAIR principles as defined by Willkinson et al (1) . The main output is a global rating, indicating the global FAIRness of the dataset. It provides implementations of the FORCE 11 FAIR data principles . 2 - FDMM (FAIR Data Maturity Model) (2) FDMM is a recognized and endorsed working group within RDA (Research Data Alliance). They produce a document that describes a maturity model for FAIR assessment with assessment indicators, priorities and evaluation methods, useful for the normalisation of assessment approaches to enable comparison of their results. 3 - SHARC (Sharing Rewards and Credit) (3) SHARC is a recognized and endorsed working group within RDA (Research Data Alliance). They produce a document that allows assessing FAIRness of projects and related human processes by either external evaluators or the researchers themselves, implying to implement simple FAIRness assessment in various communities and identify procedures and training that must be deployed and adapted to their practices and level of understanding. Results \u00b6 5 \u2605 Data Rating Tool https://oznome.csiro.au/5star/?view=5ec2a9654d0983adde57a21e FDMM (FAIR Data Maturity Model) https://drive.google.com/file/d/1a520Cbu8bryEeZIPI3h1l6zkaO7MZ39-/view?usp=sharing\" SHARC (Sharing Rewards and Credit) https://drive.google.com/file/d/1uif-jy9QBno_WPnpGL14LFpDzL366tMH/view?usp=sharing Synthesis of FAIR evaluation grids applied to the Frim dataset The Fair Data Maturity Model (FDMM) document (A) describes a maturity model for the FAIR assessment with indicators, priorities and assessment methods, which are useful for standardizing assessment approaches in order to allow comparison of their results. Whereas the FAIR SHARC (SHAring Rewards and Credit) (B) document allows the fairness of projects and associated human processes to be assessed, either by external evaluators or by the researchers themselves. Therefore, these grids cannot be compared with each other, but rather complement each other. Summary table of essential FAIR criteria based on force11.org, applied to the Frim dataset References \u00b6 Wilkinson, M., Dumontier, M., Aalbersberg, I. et al. (2016) The FAIR Guiding Principles for scientific data management and stewardship. Sci Data 3, 160018. doi:10.1038/sdata.2016.18 RDA FAIR Data Maturity Model Working Group (2020). FAIR Data Maturity Model: specification and guidelines. Research Data Alliance. DOI: 10.15497/RDA00045 Romain David, Laurence Mabile, Alison Specht, Stryeck, Sarah, Mogens Thomsen, et al. FAIRness Literacy: the Achilles' Heel of applying FAIR Principles. 2020. https://hal.archives-ouvertes.fr/hal-02483307","title":"FAIR_on_frim"},{"location":"fair/fair-frim/#odam-deployment-and-users-guide","text":"","title":"ODAM: Deployment and User's Guide"},{"location":"fair/fair-frim/#fair-grids-applied-on-frim-dataset","text":"Three FAIR grids avery different from each other. 1- 5 \u2605 Data Rating Tool From OZONOME , it aims to carry out an evaluation based on the FAIR principles as defined by Willkinson et al (1) . The main output is a global rating, indicating the global FAIRness of the dataset. It provides implementations of the FORCE 11 FAIR data principles . 2 - FDMM (FAIR Data Maturity Model) (2) FDMM is a recognized and endorsed working group within RDA (Research Data Alliance). They produce a document that describes a maturity model for FAIR assessment with assessment indicators, priorities and evaluation methods, useful for the normalisation of assessment approaches to enable comparison of their results. 3 - SHARC (Sharing Rewards and Credit) (3) SHARC is a recognized and endorsed working group within RDA (Research Data Alliance). They produce a document that allows assessing FAIRness of projects and related human processes by either external evaluators or the researchers themselves, implying to implement simple FAIRness assessment in various communities and identify procedures and training that must be deployed and adapted to their practices and level of understanding.","title":"FAIR grids applied on FRIM dataset"},{"location":"fair/fair-frim/#results","text":"5 \u2605 Data Rating Tool https://oznome.csiro.au/5star/?view=5ec2a9654d0983adde57a21e FDMM (FAIR Data Maturity Model) https://drive.google.com/file/d/1a520Cbu8bryEeZIPI3h1l6zkaO7MZ39-/view?usp=sharing\" SHARC (Sharing Rewards and Credit) https://drive.google.com/file/d/1uif-jy9QBno_WPnpGL14LFpDzL366tMH/view?usp=sharing Synthesis of FAIR evaluation grids applied to the Frim dataset The Fair Data Maturity Model (FDMM) document (A) describes a maturity model for the FAIR assessment with indicators, priorities and assessment methods, which are useful for standardizing assessment approaches in order to allow comparison of their results. Whereas the FAIR SHARC (SHAring Rewards and Credit) (B) document allows the fairness of projects and associated human processes to be assessed, either by external evaluators or by the researchers themselves. Therefore, these grids cannot be compared with each other, but rather complement each other. Summary table of essential FAIR criteria based on force11.org, applied to the Frim dataset","title":"Results"},{"location":"fair/fair-frim/#references","text":"Wilkinson, M., Dumontier, M., Aalbersberg, I. et al. (2016) The FAIR Guiding Principles for scientific data management and stewardship. Sci Data 3, 160018. doi:10.1038/sdata.2016.18 RDA FAIR Data Maturity Model Working Group (2020). FAIR Data Maturity Model: specification and guidelines. Research Data Alliance. DOI: 10.15497/RDA00045 Romain David, Laurence Mabile, Alison Specht, Stryeck, Sarah, Mogens Thomsen, et al. FAIRness Literacy: the Achilles' Heel of applying FAIR Principles. 2020. https://hal.archives-ouvertes.fr/hal-02483307","title":"References"},{"location":"json-schema/odam-data-package/","text":"{ \"$schema\" : \"http://json-schema.org/draft-07/schema#\" , \"$id\" : \"https://inrae.github.io/ODAM/json-schema/odam-data-package.json\" , \"title\" : \"odam-data-package\" , \"description\" : \"Description of an ODAM data package\" , \"type\" : \"object\" , \"definitions\" : { \"profile\" : { \"title\" : \"odam-data-package\" , \"description\" : \"odam-data-package profile is an extension of the tabular-data-package profile\" , \"type\" : \"string\" }, \"path\" : { \"title\" : \"Path\" , \"description\" : \"A fully qualified URL, or a POSIX file path..\" , \"type\" : \"string\" , \"pattern\" : \"^(?=^[^./~])(^((?!\\\\.{2}).)*$).*$\" , \"context\" : \"Implementations need to negotiate the type of path provided, and dereference the data accordingly.\" }, \"name\" : { \"title\" : \"Name\" , \"description\" : \"An identifier string. Lower case characters with `_` are allowed.\" , \"type\" : \"string\" , \"pattern\" : \"^([a-zA-Z0-9_])+$\" , \"context\" : \"This is ideally a url-usable and human-readable name. Name `SHOULD` be invariant, meaning it `SHOULD NOT` change when its parent descriptor is updated.\" }, \"title\" : { \"title\" : \"Title\" , \"description\" : \"A human-readable title.\" , \"type\" : \"string\" }, \"keywords\" : { \"title\" : \"Keywords\" , \"description\" : \"A list of keywords that describe this package.\" , \"type\" : \"array\" , \"minItems\" : 1 , \"items\" : { \"type\" : \"string\" } }, \"license\" : { \"title\" : \"License\" , \"description\" : \"A license for this descriptor.\" , \"type\" : \"object\" , \"properties\" : { \"name\" : { \"title\" : \"Open Definition license identifier\" , \"description\" : \"MUST be an Open Definition license identifier, see http://licenses.opendefinition.org/\" , \"type\" : \"string\" , \"pattern\" : \"^([-a-zA-Z0-9._])+$\" }, \"path\" : { \"$ref\" : \"#/definitions/path\" }, \"title\" : { \"$ref\" : \"#/definitions/title\" } }, \"context\" : \"Use of this property does not imply that the person was the original creator of, or a contributor to, the data in the descriptor, but refers to the composition of the descriptor itself.\" }, \"licenses\" : { \"title\" : \"Licenses\" , \"description\" : \"The license(s) under which this package is published.\" , \"type\" : \"array\" , \"minItems\" : 1 , \"items\" : { \"$ref\" : \"#/definitions/license\" }, \"context\" : \"This property is not legally binding and does not guarantee that the package is licensed under the terms defined herein.\" }, \"tabularDataResources\" : { \"title\" : \"Tabular Data Resources\" , \"description\" : \"An `array` of Tabular Data Resource objects, each compliant with the [Tabular Data Resource](/tabular-data-resource/) specification.\" , \"type\" : \"array\" , \"minItems\" : 1 , \"items\" : { \"$ref\" : \"https://inrae.github.io/ODAM/json-schema/odam-data-resource.json\" } } }, \"properties\" : { \"profile\" : { \"$ref\" : \"#/definitions/profile\" }, \"name\" : { \"$ref\" : \"#/definitions/name\" }, \"datapackage_version\" : { \"type\" : \"string\" , \"pattern\" : \"^([0-9.])+$\" }, \"keywords\" : { \"$ref\" : \"#/definitions/keywords\" }, \"licenses\" : { \"$ref\" : \"#/definitions/licenses\" }, \"resources\" : { \"$ref\" : \"#/definitions/tabularDataResources\" } }, \"required\" : [ \"profile\" , \"name\" , \"licenses\" , \"resources\" ] }","title":"Odam data package"},{"location":"json-schema/odam-data-resource/","text":"{ \"$schema\" : \"http://json-schema.org/draft-07/schema#\" , \"$id\" : \"https://inrae.github.io/ODAM/json-schema/odam-data-resource.json\" , \"title\" : \"odam-data-resource\" , \"description\" : \"Description of an ODAM data resource\" , \"type\" : \"object\" , \"definitions\" : { \"profile\" : { \"title\" : \"odam-data-resource\" , \"description\" : \"odam-data-resource profile is an extension of the tabular data resource profile\" , \"type\" : \"string\" }, \"name\" : { \"title\" : \"Name\" , \"description\" : \"An identifier string. Lower case characters with `_` are allowed.\" , \"type\" : \"string\" , \"pattern\" : \"^([a-zA-Z0-9_])+$\" , \"context\" : \"This is ideally a url-usable and human-readable name. Name `SHOULD` be invariant, meaning it `SHOULD NOT` change when its parent descriptor is updated.\" }, \"title\" : { \"title\" : \"Title\" , \"description\" : \"A human-readable title.\" , \"type\" : \"string\" }, \"description\" : { \"title\" : \"Description\" , \"description\" : \"A text description. Markdown is encouraged.\" , \"type\" : \"string\" }, \"path\" : { \"title\" : \"Path\" , \"description\" : \"A fully qualified URL, or a POSIX file path..\" , \"type\" : \"string\" , \"pattern\" : \"^(?=^[^./~])(^((?!\\\\.{2}).)*$).*$\" , \"context\" : \"Implementations need to negotiate the type of path provided, and dereference the data accordingly.\" }, \"anyTermId\" : { \"title\" : \"CV Term id\" , \"description\" : \"URL of the controlled vocabulary\" , \"type\" : \"string\" }, \"anyTermName\" : { \"title\" : \"CV Term Name\" , \"description\" : \"Short description of the controlled vocabulary\" , \"type\" : \"string\" }, \"csvDialect\" : { \"title\" : \"CSV Dialect\" , \"description\" : \"The CSV dialect descriptor.\" , \"required\" : [ \"delimiter\" , \"doubleQuote\" ], \"properties\" : { \"csvddfVersion\" : { \"$ref\" : \"#/definitions/csvddfVersion\" }, \"delimiter\" : { \"$ref\" : \"#/definitions/delimiter\" }, \"doubleQuote\" : { \"$ref\" : \"#/definitions/doubleQuote\" }, \"lineTerminator\" : { \"$ref\" : \"#/definitions/lineTerminator\" }, \"quoteChar\" : { \"$ref\" : \"#/definitions/quoteChar\" }, \"escapeChar\" : { \"$ref\" : \"#/definitions/escapeChar\" }, \"skipInitialSpace\" : { \"$ref\" : \"#/definitions/skipInitialSpace\" }, \"header\" : { \"$ref\" : \"#/definitions/header\" }, \"commentChar\" : { \"$ref\" : \"#/definitions/commentChar\" }, \"caseSensitiveHeader\" : { \"$ref\" : \"#/definitions/caseSensitiveHeader\" } } }, \"csvddfVersion\" : { \"title\" : \"CSV Dialect schema version\" , \"description\" : \"A number to indicate the schema version of CSV Dialect. Version 1.0 was named CSV Dialect Description Format and used different field names.\" , \"type\" : \"number\" , \"default\" : 1.2 }, \"delimiter\" : { \"title\" : \"Delimiter\" , \"description\" : \"A character sequence to use as the field separator.\" , \"type\" : \"string\" , \"default\" : \",\" }, \"doubleQuote\" : { \"title\" : \"Double Quote\" , \"description\" : \"Specifies the handling of quotes inside fields.\" , \"context\" : \"If Double Quote is set to true, two consecutive quotes must be interpreted as one.\" , \"type\" : \"boolean\" , \"default\" : true }, \"lineTerminator\" : { \"title\" : \"Line Terminator\" , \"description\" : \"Specifies the character sequence that must be used to terminate rows.\" , \"type\" : \"string\" , \"default\" : \"\\n\" }, \"quoteChar\" : { \"title\" : \"Quote Character\" , \"description\" : \"Specifies a one-character string to use as the quoting character.\" , \"type\" : \"string\" , \"default\" : \"\\\"\" }, \"escapeChar\" : { \"title\" : \"Escape Character\" , \"description\" : \"Specifies a one-character string to use as the escape character.\" , \"type\" : \"string\" }, \"skipInitialSpace\" : { \"title\" : \"Skip Initial Space\" , \"description\" : \"Specifies the interpretation of whitespace immediately following a delimiter. If false, whitespace immediately after a delimiter should be treated as part of the subsequent field.\" , \"type\" : \"boolean\" , \"default\" : true }, \"header\" : { \"title\" : \"Header\" , \"description\" : \"Specifies if the file includes a header row, always as the first row in the file.\" , \"type\" : \"boolean\" , \"default\" : true }, \"commentChar\" : { \"title\" : \"Comment Character\" , \"description\" : \"Specifies a character sequence causing the rest of the line after it to be ignored.\" , \"type\" : \"string\" }, \"caseSensitiveHeader\" : { \"title\" : \"Case Sensitive Header\" , \"description\" : \"Specifies if the case of headers is meaningful.\" , \"context\" : \"Use of case in source CSV files is not always an intentional decision. For example, should \\\"CAT\\\" and \\\"Cat\\\" be considered to have the same meaning.\" , \"type\" : \"boolean\" , \"default\" : false }, \"format\" : { \"title\" : \"Format\" , \"description\" : \"The file format of this resource.\" , \"context\" : \"`csv`, `xls`, `json` are examples of common formats.\" , \"type\" : \"string\" }, \"mediatype\" : { \"title\" : \"Media Type\" , \"description\" : \"The media type of this resource. Can be any valid media type listed with [IANA](https://www.iana.org/assignments/media-types/media-types.xhtml).\" , \"type\" : \"string\" , \"pattern\" : \"^(.+)/(.+)$\" }, \"encoding\" : { \"title\" : \"Encoding\" , \"description\" : \"The file encoding of this resource.\" , \"type\" : \"string\" , \"default\" : \"utf-8\" }, \"tableSchema\" : { \"title\" : \"Table Schema\" , \"description\" : \"A Table Schema for this resource, compliant with the [Table Schema](/tableschema/) specification.\" , \"type\" : \"object\" , \"required\" : [ \"fields\" , \"categories\" ], \"properties\" : { \"fields\" : { \"type\" : \"array\" , \"minItems\" : 1 , \"items\" : { \"$ref\" : \"#/definitions/tableSchemaField\" }, \"description\" : \"An `array` of Table Schema Field objects.\" }, \"primaryKey\" : { \"$ref\" : \"#/definitions/tableSchemaPrimaryKey\" }, \"foreignKeys\" : { \"type\" : \"array\" , \"minItems\" : 1 , \"items\" : { \"$ref\" : \"#/definitions/tableSchemaForeignKey\" } }, \"categories\" : { \"type\" : \"array\" , \"minItems\" : 1 , \"items\" : { \"$ref\" : \"#/definitions/tableSchemaCategory\" }, \"description\" : \"An `array` of Table Schema Category objects.\" } } }, \"tableSchemaField\" : { \"title\" : \"Table Schema Field\" , \"type\" : \"object\" , \"required\" : [ \"name\" , \"type\" , \"title\" ], \"properties\" : { \"name\" : { \"$ref\" : \"#/definitions/name\" }, \"type\" : { \"type\" : \"string\" , \"enum\" : [ \"number\" , \"string\" ] }, \"title\" : { \"$ref\" : \"#/definitions/title\" }, \"unit\" : { \"type\" : \"string\" }, \"cv_term_id\" : { \"$ref\" : \"#/definitions/anyTermId\" }, \"cv_term_name\" : { \"$ref\" : \"#/definitions/anyTermName\" }, \"constraints\" : { \"title\" : \"Constraints\" , \"description\" : \"The following constraints are supported for `string` fields.\" , \"type\" : \"object\" , \"properties\" : { \"required\" : { \"type\" : \"boolean\" , \"description\" : \"Indicates whether a property must have a value for each instance.\" , \"context\" : \"An empty string is considered to be a missing value.\" }, \"unique\" : { \"type\" : \"boolean\" , \"description\" : \"When `true`, each value for the property `MUST` be unique.\" } } } } }, \"tableSchemaPrimaryKey\" : { \"oneOf\" : [ { \"type\" : \"array\" , \"minItems\" : 1 , \"uniqueItems\" : true , \"items\" : { \"type\" : \"string\" } }, { \"type\" : \"string\" } ], \"description\" : \"A primary key is a field name or an array of field names, whose values `MUST` uniquely identify each row in the table.\" }, \"tableSchemaForeignKey\" : { \"title\" : \"Table Schema Foreign Key\" , \"description\" : \"Table Schema Foreign Key\" , \"type\" : \"object\" , \"required\" : [ \"fields\" , \"reference\" ], \"properties\" : { \"fields\" : { \"type\" : \"string\" , \"description\" : \"Fields that make up the primary key.\" }, \"reference\" : { \"type\" : \"object\" , \"required\" : [ \"resource\" , \"fields\" ], \"properties\" : { \"resource\" : { \"type\" : \"string\" , \"default\" : \"\" }, \"fields\" : { \"type\" : \"string\" } } } } }, \"tableSchemaCategory\" : { \"title\" : \"Table Schema Category\" , \"type\" : \"object\" , \"properties\" : { \"name\" : { \"type\" : \"string\" , \"enum\" : [ \"identifier\" , \"factor\" , \"quantitative\" , \"qualitative\" ] }, \"fields\" : { \"type\" : \"array\" } } }, \"tableSchemaMissingValues\" : { \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" }, \"default\" : [ \"\" ] } }, \"properties\" : { \"path\" : { \"$ref\" : \"#/definitions/path\" }, \"profile\" : { \"$ref\" : \"#/definitions/profile\" }, \"name\" : { \"$ref\" : \"#/definitions/name\" }, \"title\" : { \"$ref\" : \"#/definitions/title\" }, \"identifier\" : { \"description\" : \"name of the identifier field (column name). i.e same as primaryKey\" , \"type\" : \"string\" }, \"obtainedFrom\" : { \"description\" : \"name of the file \" , \"type\" : \"string\" }, \"joinkey\" : { \"description\" : \"name of the identifier field (column name) that serves to link with its father file i.e same as foreignKey\" , \"type\" : \"string\" }, \"cv_term_id\" : { \"$ref\" : \"#/definitions/anyTermId\" }, \"cv_term_name\" : { \"$ref\" : \"#/definitions/anyTermName\" }, \"schema\" : { \"$ref\" : \"#/definitions/tableSchema\" }, \"dialect\" : { \"$ref\" : \"#/definitions/csvDialect\" }, \"encoding\" : { \"$ref\" : \"#/definitions/encoding\" }, \"format\" : { \"$ref\" : \"#/definitions/format\" }, \"mediatype\" : { \"$ref\" : \"#/definitions/mediatype\" }, \"missingValues\" : { \"$ref\" : \"#/definitions/tableSchemaMissingValues\" } }, \"required\" : [ \"path\" , \"name\" , \"title\" , \"identifier\" , \"schema\" , \"dialect\" , \"format\" ] }","title":"Odam data resource"}]}